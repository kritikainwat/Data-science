# -*- coding: utf-8 -*-
"""Employee attrition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z9L3k46VoOf_dMZb-w4TeSj8Xnc52uek

# **Employee Attrition preditiction: Machine learning model**

# Import Libraries
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

data=pd.read_csv("/content/Dataset01-Employee_Attrition.csv")
data.head()

data.shape
data.columns
data.dtypes
data.info()
data.describe()

data.drop_duplicates(inplace=True)
data.shape

data1=data.drop_duplicates()
data1.shape

"""# Check for missing ***value***"""

data.isnull().sum()

"""**Data exploration and visualization** *italicized text*"""

data1['left'].value_counts()
data1["left"].value_counts().plot(kind="bar")
data1.head()

print(data1.columns)

pd.crosstab(data1.salary,data1.left).plot(kind="bar")

pd.crosstab(data1.Department,data1.left).plot(kind="bar")

pd.crosstab(data1.Department,data1.left)

pd.crosstab(data1.salary,data1.left)

num_feature_list1=[f for f in data1.columns if data1.dtypes[f]=='float64']
num_feature_list1

num_feature_list2=[f for f in data1.columns if data1.dtypes[f]=='int64']
num_feature_list2

fig,axes=plt.subplots(ncols= 4,figsize=(12,3))
for column,axis in zip (num_feature_list1[:4],axes):
  sns.boxplot(data=data1[column],ax=axis)
  axis.set_title(column)
  plt.tight_layout()
  plt.show()

fig,axes=plt.subplots(ncols= 3,figsize=(12,3))
for column,axis in zip (num_feature_list1[:4],axes):
  sns.boxplot(data=data1[column],ax=axis)
  axis.set_title(column)
  plt.tight_layout()
  plt.show()

data1['number_project'].plot(kind='hist',bins=5)

data1['average_montly_hours'].plot(kind='hist',bins=6)

data1['time_spend_company'].plot(kind='hist',bins=5)

data1['satisfaction_level'].plot(kind='hist',bins=5)

data1['last_evaluation'].plot(kind='hist',bins=5)

"""**Feature Engineering**

# Labell Encoding:Converting categorical features into Numerical
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
Label_encoder=LabelEncoder()

data=pd.read_csv("/content/Dataset01-Employee_Attrition.csv")
data.head()

from google.colab import drive
drive.mount('kizie')

Label_encoder=LabelEncoder()
data['salary']=Label_encoder.fit_transform(data['salary'])
data['DeprecationWarning']=Label_encoder.fit_transform(data['Department'])

data.head()

x=data.drop('left',axis=1)
y=data['left']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

x_train.shape

X_train.head()

import pandas as pd
 from sklearn.preprocessing import StandardScaler
 from sklearn.model_selection import train_test_split
 std_scaaler=StandardScaler

import pandas as pd

# For One-Hot Encoding
x_train_encoded = pd.get_dummies(x_train, drop_first=True)
x_test_encoded = pd.get_dummies(x_test, drop_first=True)

# Align columns in x_test with x_train
x_test_encoded = x_test_encoded.reindex(columns=x_train_encoded.columns, fill_value=0)

from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
xtrain_scaled = std_scaler.fit_transform(x_train_encoded)
xtest_scaled = std_scaler.transform(x_test_encoded)

xtrain_scaled
xtest_scaled

from sklearn.ensemble import RandomForestClassifier

Random_forest_model=RandomForestClassifier()

Random_forest_model.fit(xtrain_scaled,y_train)

"""Model Training:"""

Random_forest_model.fit(xtrain_scaled,y_train)

"""Model Prediction:"""

y_pred=Random_forest_model.predict(xtest_scaled)
y_pred

"""**Model Evaluation:*"""

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)
print(cm)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'cm' is your confusion matrix
sns.heatmap(cm, annot=True, fmt="d")

plt.show()  # This line will display the heatmap

from sklearn.metrics import top_k_accuracy_score
model= accuracy =  top_k_accuracy_score(y_test,y_pred)
print("Accuracy of the model=", top_k_accuracy_score)



from sklearn.metrics import precision_score
model= precision =  precision_score(y_test,y_pred)
print("precision of the model=", precision_score)

from sklearn.metrics import recall_score
model= recall =  recall_score(y_test,y_pred)
print("recall of the model=", recall_score)

from sklearn.metrics import f1_score
model= recall =  f1_score(y_test,y_pred)
print("f1 of the model=", f1_score)

"""CLASSIFICATION Report"""

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

"""feature Importance Metrix in Random Forest/feature Extraction:"""

import pandas as pd

score_list = Random_forest_model.feature_importances_
list_of_features = list(X.columns)

# Check if the lengths of the two lists are equal
# If not, print the lengths and investigate the cause of the mismatch
if len(score_list) != len(list_of_features):
    print(f"Length of score_list: {len(score_list)}")
    print(f"Length of list_of_features: {len(list_of_features)}")
    # You might need to debug your feature selection process or model training
    # to ensure the number of features is consistent
else:
    score_df = pd.DataFrame({"feature": list_of_features, "Score": score_list})
    # 'ascending' is the correct spelling.
    # 'score' column should be 'Score' to match the column name in the DataFrame.
    score_df = score_df.sort_values(by='Score', ascending=False)
    print(score_df)



import pandas as ps
import numpy as np
list_of_features=list(X.columns)
plt.figure(figsize=(8,6))
plt.barh(range(len(list_of_features)),Random_forest_model.feature_importances_)
plt.yticks(np.range(len(list_of_features)),list_of_features)
plt.ylabel('features')
plt.xlabel('feature importance')
plt.title('features importance')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

list_of_features = list(X.columns)
importances = Random_forest_model.feature_importances_

# Check the length of the importances and features
if len(list_of_features) != len(importances):
    print("Warning: Mismatch between features and importances.")
else:
    plt.figure(figsize=(8, 6))
    plt.barh(range(len(list_of_features)), importances)
    plt.yticks(np.arange(len(list_of_features)), list_of_features)
    plt.ylabel('Features')
    plt.xlabel('Feature Importance')
    plt.title('Feature Importance')
    plt.show()

"""K-fold Cross Validation:

"""

# Applying 5-fold Cross Validation

